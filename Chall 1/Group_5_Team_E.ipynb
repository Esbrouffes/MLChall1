{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries: Standard ones\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Library for boxplots\n",
    "import seaborn as sns\n",
    "\n",
    "# K-means function\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Functions for silhouette\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "# Function to standardize the data \n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# Functions for hierarchical clustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "\n",
    "\n",
    "#for map \n",
    "import folium\n",
    "from folium.plugins import FastMarkerCluster\n",
    "from folium.vector_layers import CircleMarker\n",
    "import branca.colormap as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The first step is to visualize the data in order to understand it. Let's show a table.\n",
    "temperature=pd.read_csv(\"temperat.csv\", sep=\";\")\n",
    "temperature=temperature.rename(columns={'Unnamed: 0': 'Ville'}) # renaming the first column\n",
    "temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The second step is to draw some statistics from the data.\n",
    "temperature.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's present these results on a boxplot using Seasborne, as there is still a lot of data. \n",
    "plt.figure()\n",
    "plt.title(\"boxplot distribution for the temperature\")\n",
    "temperature_boxplot=sns.boxplot(data=temperature.drop([\"Moyenne\", \"Amplitude\",\"Latitude\",\"Longitude\",\"Region\"], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Represented like this, we can't extract clusters, so let's represent it differently\n",
    "value_to_visualize = input(\"What value do you want to visualize ( Amplitude, Moyenne, etc) ? \\n\") #so we can explore the data better\n",
    "plt.figure()\n",
    "plt.title(\"2D visualization of the data : Latitude / Longitude\")\n",
    "plt.scatter(temperature[\"Latitude\"], temperature[\"Longitude\"],c=temperature[value_to_visualize]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's now present the temperature according to the city\n",
    "plt.figure()\n",
    "plt.title(\"2D visualization of the data : Latitude / Longitude\")\n",
    "plt.scatter(temperature[\"Ville\"], temperature[value_to_visualize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now display all the points on a map coloured by the field we selected above\n",
    "file=temperature\n",
    "field_used=value_to_visualize\n",
    "\n",
    "map = folium.Map(tiles='cartodbpositron', zoom_start=3 ,location=[48.499998 ,23.3833318])\n",
    "\n",
    "linear=cm.linear.RdYlBu_06.scale(0, 1)\n",
    "\n",
    "for index,row in file.iterrows():\n",
    "#     keyon_cluster = folium.Marker(location=[row['Latitude'],row['Longitude']],popup=row['Ville']).add_to(m)\n",
    "    folium.CircleMarker(location=[row['Latitude'],row['Longitude']], \n",
    "                        radius=5,\n",
    "                        popup=str(row['Ville'])+': '+str(row[field_used]), \n",
    "                        line_color=None,\n",
    "                        fill_color=linear(1-(row[field_used]-file[field_used].min())/(file[field_used].max()-file[field_used].min())),\n",
    "                        fill_opacity=1,\n",
    "                        color=linear(1-(row[field_used]-file[field_used].min())/(file[field_used].max()-file[field_used].min()))).add_to(map)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We can see the results are relevant and correspond to what we expected. \n",
    " # Let's now see the data of the mean temperature by region \n",
    "\n",
    "plt.figure()\n",
    "plt.title('2D representation of temperature dataset')\n",
    "plt.scatter(temperature['Region'], temperature[field_used]) #or change by Moyenne\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that for different fields ( Moyenne / Amplitude), there are a lot of disparities in the given regions.\n",
    "# Let's use our knowledge in clustering to find clusters to regroup cities with similar caracteristics.\n",
    "\n",
    "\n",
    "#Let's try to find clusters: \n",
    "plt.figure()\n",
    "\n",
    "new_temp = temperature.drop(['Ville','Region','Latitude','Longitude',\"Amplitude\",\"Moyenne\" ], axis=1)\n",
    "\n",
    "kmeans = KMeans(n_clusters=4,n_init=10,init='random').fit(new_temp) #trying to find 4 clusters, 4 being a purely aritrairy number\n",
    "centers=kmeans.cluster_centers_\n",
    "kmeans.labels_\n",
    "plt.scatter(temperature['Latitude'], temperature['Longitude'],c=kmeans.labels_) # === draw on map = better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Let's do the Elbow method to find the optimal number of clusters, using the silhouette score to compute the cost\n",
    "range_n_clusters = range(2,15) #trying to find the optimal number of clusters which suits best our dataset ( that has \"30~cities\")\n",
    "sil_score = []\n",
    "\n",
    "for n in range_n_clusters:\n",
    "    sil_mean=[]\n",
    "    for i in range(20): # so we compute it 20 times and take the mean to know exatly wich number of clusters would be better\n",
    "        clusterer = KMeans(n_clusters=n, n_init=13, init = 'random').fit(new_temp) # Clustering with the given number of clusters\n",
    "        cluster_labels = clusterer.labels_ # Extract the labels of each cluster\n",
    "        silhouette_avg = silhouette_score(new_temp, cluster_labels) # Corresponding silhouette score\n",
    "        sil_mean.append(silhouette_avg) # Adding the silhouette score corresponding to the number of clusters to our list so \n",
    "    sil_score.append(np.mean(sil_mean))                      # we can print it afterward\n",
    "    #print(\"For n =\", n, \"Silhouette_score:\", silhouette_avg) \n",
    "plt.figure()\n",
    "plt.title('Elbow method to find the optimal number of clusters')\n",
    "plt.scatter(range_n_clusters, sil_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When running several times the function below we get 5 or sometimes 6 clusters as the optimal number. We will continue with 5 ! \n",
    "# but first, let's see what happens when we use 5 clusters and visualize it on the map, coloring the cities of one cluster with the same color\n",
    "kmeans = KMeans(n_clusters=5,n_init=10,init='random').fit(new_temp)\n",
    "\n",
    "new_temp_with_labels=pd.concat([temperature,pd.DataFrame(data=kmeans.labels_,columns=['label'])], axis=1, sort=False)\n",
    "#We select the file we're going to use\n",
    "file=new_temp_with_labels\n",
    "field_used='label'\n",
    "\n",
    "linear=cm.linear.RdYlBu_06.scale(0, 1)\n",
    "\n",
    "\n",
    "map2 = folium.Map(tiles='cartodbpositron', zoom_start=3 ,location=[48.499998 ,23.3833318])\n",
    "\n",
    "for index,row in file.iterrows():\n",
    "#     keyon_cluster = folium.Marker(location=[row['Latitude'],row['Longitude']],popup=row['Ville']).add_to(m)\n",
    "    folium.CircleMarker(location=[row['Latitude'],row['Longitude']], \n",
    "                        radius=5,\n",
    "                        popup=str(row['Ville'])+': '+str(row[field_used]), \n",
    "                        line_color=None,\n",
    "                        fill_color=linear(1-(row[field_used]-file[field_used].min())/(file[field_used].max()-file[field_used].min())),\n",
    "                        fill_opacity=1,\n",
    "                        color=linear(1-(row[field_used]-file[field_used].min())/(file[field_used].max()-file[field_used].min()))).add_to(map2)\n",
    "map2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" We can see that proximity in term of distance is often related to proximity in terms of temperature, which is logic, but some points have the same climate and are very far away.\n",
    "That means there is another factor that creates the climate. It could be the proximity with the ocean, or altitude\n",
    "(naively : we know there are several types of climate, for example continental climate whose major caracteristic is the really high\n",
    "amplitude. Let's try to study this amplitude ! )\n",
    "\"\"\"\n",
    "## We will now try to regroup the different amplituds for the temperature and try to find which are the similar ones : we know that\n",
    "# a continental climate is when there is a huge amplitude, so let's see if it's correlated with the geographic position\n",
    "\n",
    "\n",
    "temp_amplitude=temperature[['Ville','Amplitude']]\n",
    "\n",
    "temp_ampl_no_city=temperature['Amplitude'].reset_index()\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6,7,8,9,10]\n",
    "for n in range_n_clusters:\n",
    "    kmeans_ampl = KMeans(n_clusters=n, n_init=10).fit(temp_ampl_no_city) # Clustering with the given number of clusters\n",
    "    cluster_labels = kmeans_ampl.labels_ # Extract the labels\n",
    "    silhouette_avg = silhouette_score(temp_ampl_no_city, cluster_labels) # Corresponding silhouette score\n",
    "   \n",
    "    # Display the results\n",
    "   # print(\"For n =\", n, \"Silhouette_score:\", silhouette_avg) results give 5 as best one \n",
    "kmeans_ampl = KMeans(n_clusters=5, n_init=10).fit(temp_ampl_no_city)\n",
    "y=temp_amplitude.sort_values(by='Amplitude')\n",
    "map_for_ampl=pd.concat([temperature,pd.DataFrame(data=kmeans_ampl.labels_,columns=['cluster_labels'])], axis=1, sort=False)\n",
    "field_used='cluster_labels'\n",
    "file=map_for_ampl\n",
    "map3 = folium.Map(tiles='cartodbpositron', zoom_start=3 ,location=[48.499998 ,23.3833318])\n",
    "\n",
    "for index,row in file.iterrows():\n",
    "#     keyon_cluster = folium.Marker(location=[row['Latitude'],row['Longitude']],popup=row['Ville']).add_to(m)\n",
    "    folium.CircleMarker(location=[row['Latitude'],row['Longitude']], \n",
    "                        radius=5,\n",
    "                        popup=str(row['Ville'])+': '+str(row[field_used]), \n",
    "                        line_color=None,\n",
    "                        fill_color=linear(1-(row[field_used]-file[field_used].min())/(file[field_used].max()-file[field_used].min())),\n",
    "                        fill_opacity=1,\n",
    "                        color=linear(1-(row[field_used]-file[field_used].min())/(file[field_used].max()-file[field_used].min()))).add_to(map3)\n",
    "map3  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The results aren't very simple to analyse so we can't find which type of climate a city belongs to without having \n",
    "#more data about the cities. We can however conjecture that cities that are further in land have higher amplitude \n",
    "#whereas cities close to oceans / seas have more temperate climates, with some exceptions\n",
    "#Let's \n",
    "\n",
    "\n",
    "### Let's now try to see for each group obtained with the 5 clusters obtained previously \n",
    "# if we can draw seasons from them to see how many seasons would there be\n",
    "group1=new_temp_with_labels[new_temp_with_labels['label']==0]\n",
    "group2=new_temp_with_labels[new_temp_with_labels['label']==1]\n",
    "group3=new_temp_with_labels[new_temp_with_labels['label']==2]\n",
    "group4=new_temp_with_labels[new_temp_with_labels['label']==3]\n",
    "group5=new_temp_with_labels[new_temp_with_labels['label']==4]\n",
    "\n",
    "\n",
    "#now we do k-means but to get the seasons ( focusing on group 2, this can be changed )\n",
    "\n",
    "group2_month=group2.drop(['Ville','Moyenne','Amplitude','Latitude','Longitude','Region','label'],axis=1)\n",
    "\n",
    "#We first need to transpose our dataset\n",
    "group2_month_rev=group2_month.T\n",
    "group2_month_rev_clean=group2_month_rev.reset_index().drop(['index'],axis=1)\n",
    "\n",
    "#and now we perform k-means on the reverted dataframe\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6,7,8,9,10]\n",
    "for n in range_n_clusters:\n",
    "    kmeans_seasons = KMeans(n_clusters=n, n_init=10).fit(group2_month_rev_clean) # Clustering with the given number of clusters\n",
    "    cluster_labels = kmeans_seasons.labels_ # Extract the labels\n",
    "    silhouette_avg = silhouette_score(group2_month_rev_clean, cluster_labels) # Corresponding silhouette score\n",
    "   \n",
    "    # Display the results\n",
    "   # print(\"For n =\", n, \"Silhouette_score:\", silhouette_avg)  => to find the elbow\n",
    "    \n",
    "    \n",
    "# For group 2, we should choose k=2 or k=5 \n",
    "# k=2\n",
    "\n",
    "kmeans_seasons = KMeans(n_clusters=2,n_init=10,init='random').fit(group2_month_rev_clean)\n",
    "print( \" months that are together if there were two seasons : \", kmeans_seasons.labels_ )\n",
    "\n",
    "\n",
    "\n",
    "#k=5\n",
    "kmeans_seasons = KMeans(n_clusters=5,n_init=10,init='random').fit(group2_month_rev_clean)\n",
    "print( \" months that are together if there were five seasons : \", kmeans_seasons.labels_ ) \n",
    "#group2 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Now we are going to perform hierarchical clustering\n",
    "\n",
    "# We standardize the columns of interest\n",
    "new_temp_scaled=pd.DataFrame(scale(new_temp),columns=new_temp.columns,index=new_temp.index)  # scale(food) returns a numpy array, so use pd.Dataframe to reconstruct your dataframe\n",
    "\n",
    "\n",
    "# We apply hierarchical clustering with dissimilarity measure \"ward\"\n",
    "Z = linkage(new_temp_scaled, 'ward') # 'average' is the method used to compute the distance. Metric is \"euclidian\" by default\n",
    "\n",
    "# We represent the corresponding Dendrogram\n",
    "\n",
    "#Since we want to display the Dendrogram with cities and not just index, we create the label_city list\n",
    "\n",
    "label_city=[]\n",
    "for index,row in temperature.iterrows():\n",
    "    label_city.append(row['Ville'])\n",
    "    \n",
    "#And now we can plot\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.ylabel('city')\n",
    "plt.xlabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    orientation='right',\n",
    "    labels=label_city\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "#And here is the dendrogram !! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"What could be intersteting would be to use the PCA method to see if we can run another kmeans that would be as performing,\n",
    "but withdrawing useless parameters therefore having a faster and less resource-consuming algorithm\"\"\"\n",
    "\n",
    "new_temperature = temperature.drop(temperature.columns[[0,17]], axis='columns') #on enleve la colonne des villes\n",
    "#print(new_temperature)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "tempCPA = new_temperature.values\n",
    "std_scale = StandardScaler().fit(tempCPA)\n",
    "tempCPA_scaled = std_scale.transform(tempCPA)\n",
    "#print(tempCPA_scaled)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA().fit(tempCPA_scaled[:,:12])\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "pca.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pca.transform(tempCPA_scaled[:,:12])\n",
    "\n",
    "plt.scatter(temp[:,0], temp[:,1], c=new_temperature['Moyenne'])\n",
    "plt.xlabel('1st principal component')\n",
    "plt.ylabel('2nd principal component')\n",
    "plt.xlim(-6, 7)\n",
    "plt.ylim(-3, 3)\n",
    "\n",
    "for i in range(temp.shape[0]):\n",
    "    plt.text(temp[i,0], temp[i,1], temperature[\"Ville\"][i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca.components_\n",
    "for i, (x, y) in enumerate(zip(components[0,:], components[1,:])):\n",
    "    plt.plot([0, x], [0, y], color='k')\n",
    "    plt.text(x, y, new_temperature.columns[i])\n",
    "\n",
    "plt.plot([-0.6, 0.6], [0, 0], color='grey', ls='--')\n",
    "plt.plot([0, 0], [-0.6, 0.6], color='grey', ls='--')\n",
    "\n",
    "plt.xlim(-0.5, 0.5)\n",
    "plt.ylim(-0.5, 0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the circle of correlations obtained and the graph of the position of the individuals, we can caracterize\n",
    "# cities according to their temperature. That is to say that the cities on the right side of the graph will have \n",
    "# higher temperatures than the left-hand side. Indeed all the variables have a positive correlation \n",
    "# with axis 1. In addition, the variables concerning the winter months are positively correlated with axis 2. \n",
    "# while the variables for the summer months are negatively correlated with Axis 2. Thus the \n",
    "# cities at the top of the graph will have a mild winter but a cold summer and cities at the bottom of the graph will have\n",
    "# a cold winter and a warm summer. \n",
    "# Following this resonance, Moscow is a city with low temperatures and very cold winters and \n",
    "# hot summers. In contrast to Seville which is a city where it seems to be hot all year round.\n",
    "\n",
    "\n",
    "\n",
    "_, axes = plt.subplots(ncols=4, figsize=(16,4))\n",
    "for i, (ax, col) in enumerate(zip(axes, ['Latitude', 'Longitude', 'Moyenne', 'Amplitude'])):\n",
    "    ax.scatter(temp[:,0], temperature[col])\n",
    "    ax.set_title(f'1st component vs {col}')\n",
    "    \n",
    "_, axes = plt.subplots(ncols=4, figsize=(16,4))\n",
    "\n",
    "for i, (ax, col) in enumerate(zip(axes, ['Latitude', 'Longitude', 'Moyenne', 'Amplitude'])):\n",
    "    ax.scatter(temp[:,1], temperature[col])\n",
    "    ax.set_title(f'2nd component vs {col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA().fit(tempCPA_scaled[:,:16])\n",
    "components = pca.components_\n",
    "for i, (x, y) in enumerate(zip(components[0,:12], components[1,:12])):\n",
    "    plt.plot([0, x], [0, y], color='k')\n",
    "    plt.text(x, y, new_temperature.columns[i])\n",
    "    \n",
    "for i, (x, y) in enumerate(zip(components[0,12:16], components[1,12:16])):\n",
    "    plt.plot([0, x], [0, y], color='b')\n",
    "    plt.text(x, y, new_temperature.columns[i+12])\n",
    "\n",
    "plt.plot([-0.6, 0.6], [0, 0], color='grey', ls='--')\n",
    "plt.plot([0, 0], [-0.6, 0.6], color='grey', ls='--')\n",
    "\n",
    "plt.xlim(-0.5, 0.5)\n",
    "plt.ylim(-0.5, 0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean is highly correlated with Component 1, one could even say that Component 1 is the mean. \n",
    "# because the correlation is close to 1.\n",
    "# Moreover amplitude and longitude are negatively correlated with component 2.\n",
    "# so the cities at the bottom of the graph have a strong annual amplitude like Kiev and the cities at the top of the graph do not.\n",
    "# have a low thermal amplitude like Dublin. \n",
    "\n",
    "\n",
    "\n",
    "#Accordingly to what we expected, the results of the PCA is the same as the one for the kmeans, we haven't \" lost \" any useful data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
